{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "path = \"/data/nvme6n1p1/adal_workspace/small_llm/datasets/sft_dataset_conversation\"\n",
    "df_train = pd.read_csv(f\"{path}/everyday-conversations_train-kk.csv\")\n",
    "df_test = pd.read_csv(f\"{path}/everyday-conversations_test-kk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/atune/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "ds_dict = DatasetDict({\n",
    "    \"train\":ds_train.select_columns([\"messages\"]),\n",
    "    \"test\":ds_test.select_columns([\"messages\"])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 2260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chatting_template_full(sample):\n",
    "    sample[\"text\"]=sample['messages'].replace(\"<#>\", \"<|im_start|>user:\").replace(\"<*>\", \"<|im_end|>\\n<|im_start|>assistant:\") + \"<|im_end|>\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2260/2260 [00:00<00:00, 18762.63 examples/s]\n",
      "Map: 100%|██████████| 119/119 [00:00<00:00, 24442.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_input = ds_dict.map(apply_chatting_template_full).select_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <|im_start|>user:\n",
      "Сәлеметсіз бе\n",
      "<|im_end|>\n",
      "<|im_start|>assistant:\n",
      "Сәлеметсіз бе! Бүгін мен сізге қалай көмектесе аламын?\n",
      "<|im_start|>user:\n",
      "Мен келесі демалысыма жағажай курортын іздеп жүрмін. Кейбір танымал жағажайларды ұсына аласыз ба?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant:\n",
      "Кейбір танымал жағажай курорттарына Гавайидегі Мауи, Мальдив аралдары және Багам аралдары кіреді. Олар әдемі жағажайларымен және мөлдір суларымен танымал.\n",
      "<|im_start|>user:\n",
      "Бұл керемет естіледі. Кариб теңізінде отбасылар үшін қолайлы курорттар бар ма?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant:\n",
      "Иә, Кариб теңізіндегі отбасыларға қолайлы курорттар үшін Туркс және Кайкос аралдары және Барбадос тамаша таңдау болып табылады. Олар барлық жастағы адамдарға арналған бірқатар іс-шаралар мен ыңғайлылықтарды ұсынады.\n",
      "<|im_start|>user:\n",
      "Жарайды, мен оларды қарастырамын. Ұсыныстарыңыз үшін рақмет!\n",
      "<|im_end|>\n",
      "<|im_start|>assistant:\n",
      "Қош келдіңіз. Демалыс үшін тамаша курорт табасыз деп үміттенемін.\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(ds_input['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/nvme6n1p1/adal_workspace/small_llm/models/small_lm_test1\"\n",
    "\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  237,     1,   518,   316,    42,   215, 22582,  1754, 14572,  8150,\n",
      "           215,     2,   215,     1,  1262, 24939,    42,   215, 22582,  1754,\n",
      "         14572,  8150,    17, 19726,   691, 28952,  5327,  6058,   719, 34041,\n",
      "            47,   215,     1,   518,   316,    42,   215,  8842,  5079,  8659,\n",
      "          4946,   471, 36521, 39895,  1399, 20160, 29804,   379,    30, 13514,\n",
      "          5550, 36521,  2852, 31680, 26133,   701,    47,   215,     2,   215,\n",
      "             1,  1262, 24939,    42,   215, 31024,  5550, 36521, 39895, 18142,\n",
      "         22432,   431, 35184,   531, 37462,    28, 26594, 12506, 27628,   456,\n",
      "          3747, 44306, 27628,  8840,    30,  4648, 20158, 36521, 17430,   456,\n",
      "         44413,  6770, 32967,  5550,    30,   215,     1,   518,   316,    42,\n",
      "           215,  4612, 14464,  7684,  2053,    30, 42537,  5397,   561, 40009,\n",
      "           820, 14896, 39895, 11939,   964,   835,    47,   215,     2,   215,\n",
      "             1,  1262, 24939,    42,   215, 40572,    28, 42537,  5397,  1428,\n",
      "         40434, 14896, 39895, 11939,   820,  9136,  1346,   456, 20828, 40738,\n",
      "         27628,   456, 25739,  2037, 42409, 14409,  9044,  1317,  3016,    30,\n",
      "          4648,  2541,  9798, 11140,  1902,  8773,  1733,    29, 11857,   691,\n",
      "         17641,   314,  1548,  4039,  9919,    30,   215,     1,   518,   316,\n",
      "            42,   215,  1118,   540,  1634,    28,   691,  4556, 43178,  2973,\n",
      "            30, 43034,  2492, 19016,   820,   695,   288,  3007,    17,   215,\n",
      "             2,   215,     1,  1262, 24939,    42,   215,   514,  4244,  4621,\n",
      "          7171,    30, 11510, 36262,   820, 14409, 30255, 17152,  1260,  2101,\n",
      "           898, 28239, 31300,    30,   215,     2, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
      "         49152, 49152]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "tokenized_output = tokenizer(\n",
    "    ds_input['train'][0]['text'], \n",
    "    padding=\"max_length\",  # You can also use padding=True for dynamic padding\n",
    "    truncation=True,  # Ensures the text does not exceed max length\n",
    "    max_length=512,  # Adjust based on your model\n",
    "    return_tensors=\"pt\"  # Returns PyTorch tensors (or use \"tf\" for TensorFlow)\n",
    ")\n",
    "\n",
    "print(tokenized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/atune/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configure the SFTTrainer\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    report_to=None, # To disable wandb\n",
    "    output_dir=\"/data/nvme6n1p1/adal_workspace/small_llm/models/chatting_llm/try_1\",\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.95,\n",
    "    max_seq_length=1024,\n",
    "    # max_steps=141,  # Adjust based on dataset size and desired training duration\n",
    "    per_device_train_batch_size=8,  # Set according to your GPU memory capacity\n",
    "    learning_rate=3e-6,  # Common starting point for fine-tuning\n",
    "    logging_steps=10,  # Frequency of logging training metrics\n",
    "    save_steps=100,  # Frequency of saving model checkpoints\n",
    "    evaluation_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
    "    eval_steps=100,  # Frequency of evaluation\n",
    "    num_train_epochs=2,\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  # Use MPS for mixed precision training\n",
    "    hub_model_id=finetune_name,  # Set a unique name for your model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_682649/198691415.py:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Converting train dataset to ChatML: 100%|██████████| 2260/2260 [00:00<00:00, 29030.15 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 2260/2260 [00:00<00:00, 55569.98 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 2260/2260 [00:01<00:00, 1746.52 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 2260/2260 [00:00<00:00, 4732.22 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 119/119 [00:00<00:00, 40307.05 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 119/119 [00:00<00:00, 28934.62 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 119/119 [00:00<00:00, 1674.63 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 119/119 [00:00<00:00, 4346.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds_input[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=ds_input[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamubieke-parehati\u001b[0m (\u001b[33mmamubieke-parehati-ISSAI\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/nvme6n1p1/adal_workspace/small_llm/llm_train/wandb/run-20250304_101055-kg7sp3bw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mamubieke-parehati-ISSAI/huggingface/runs/kg7sp3bw' target=\"_blank\">/data/nvme6n1p1/adal_workspace/small_llm/models/chatting_llm/try_1</a></strong> to <a href='https://wandb.ai/mamubieke-parehati-ISSAI/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mamubieke-parehati-ISSAI/huggingface' target=\"_blank\">https://wandb.ai/mamubieke-parehati-ISSAI/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mamubieke-parehati-ISSAI/huggingface/runs/kg7sp3bw' target=\"_blank\">https://wandb.ai/mamubieke-parehati-ISSAI/huggingface/runs/kg7sp3bw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='566' max='566' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [566/566 06:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.355600</td>\n",
       "      <td>1.362775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.256700</td>\n",
       "      <td>1.251368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.161300</td>\n",
       "      <td>1.210384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.173300</td>\n",
       "      <td>1.194203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.149100</td>\n",
       "      <td>1.186450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=566, training_loss=1.2867608087222904, metrics={'train_runtime': 386.8576, 'train_samples_per_second': 11.684, 'train_steps_per_second': 1.463, 'total_flos': 9979924473114624.0, 'train_loss': 1.2867608087222904})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model's inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\\nСәлем\\n\"\n",
    "\n",
    "messages = [{'role':'user', 'content':prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt_2 = formatted_prompt.replace(\"user : \", \"user:\\n\") + \"<|im_start|>assistant:\\n\"\n",
    "\n",
    "inputs_2 = tokenizer(formatted_prompt_2, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "\n",
      "Сәлем\n",
      "<|im_end|>\n",
      "<|im_start|>assistant:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(formatted_prompt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 215, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|im_end|>\\n<|im_start|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria\n",
    "# https://github.com/huggingface/trl/issues/921\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, eos_sequence = [2, 215, 1]):\n",
    "        self.eos_sequence = eos_sequence\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "        return self.eos_sequence in last_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.34it/s]\n"
     ]
    }
   ],
   "source": [
    "new_model = LlamaForCausalLM.from_pretrained(\"/data/nvme6n1p1/adal_workspace/small_llm/models/chatting_llm/try_1/checkpoint-566\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "streamer = TextStreamer(tokenizer)\n",
    "ouputs  = new_model.generate(**inputs_2,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=100, \n",
    "    temperature=0.3,\n",
    "    stopping_criteria=[EosListStoppingCriteria()],\n",
    "    # streamer=streamer, \n",
    "    repetition_penalty=1.17, \n",
    "    # top_p=1.0,\n",
    "    eos_token_id=tokenizer.encode(\"<|im_start|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "\n",
      "Сәлем\n",
      "<|im_end|>\n",
      "<|im_start|>assistant:\n",
      "Қош келдіңіз! );- деп жауап берді. | (қос нүктеден кейін) және теруді жалғастырыңыз). Бұл көмектеседі, рахмет; қажет емес.)>><h2 ><p class=\"title\" id=\"\" name = \"Мысықтың аты\"></a href_lengths` </heading style “Tail Length”) # Мысалдар мен кеңестер үшін атауды пайдаланыңыз немесе оны өзгертіңіз): мысықтар туралы\n"
     ]
    }
   ],
   "source": [
    "ouputs_2  = new_model.generate(**inputs_2, max_new_tokens=100, do_sample=True, eos_token_id=tokenizer.encode(\"<|im_end|>\"), repetition_penalty=2.07, temperature=0.1, top_p=1.0, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "from typing import Literal, Optional\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from transformers import LlamaConfig as HFLlamaConfig\n",
    "\n",
    "TEST_PROMPT = \"Қазақстан туралы не білесін?\"\n",
    "\n",
    "def check_converted_model_generation(save_path: Path):\n",
    "    \"\"\"Loads a huggingface model and tokenizer from `save_path` and\n",
    "    performs a dummy text generation.\"\"\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "    messages = [{'role':'user', 'content':TEST_PROMPT}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    formatted_prompt_2 = formatted_prompt.replace(\"user : \", \"user:\\n\") + \"<|im_start|>assistant:\\n\"\n",
    "    input_ids = tokenizer(formatted_prompt_2, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "    print(\"Inputs:\", tokenizer.batch_decode(input_ids))\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(save_path).cuda().bfloat16()\n",
    "    out = model.generate(input_ids, max_new_tokens=1000, do_sample=True, temperature=0.3, top_p=0.98, repetition_penalty=1.17)\n",
    "    \n",
    "    print(\"Generation (converted): \", tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    # print(\"Generation (Not-converted): \", out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: ['<|im_start|>user\\n«Қазақстан отар болып келді және солай болып қалды» деп айтқан қайраткер?<|im_end|>\\n<|im_start|>assistant:\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation (converted):  user\n",
      "«Қазақстан отар болып келді және солай болып қалды» деп айтқан қайраткер?\n",
      "assistant:\n",
      "Ол 1920 жылдары Кеңес өкіметіне қарсы шыққан қазақ жастарын қолдаған, бірақ кейін ол «халық жауы» деген айыппен тұтқындалып, ату жазасына кесілген.\n"
     ]
    }
   ],
   "source": [
    "TEST_PROMPT = \"«Қазақстан отар болып келді және солай болып қалды» деп айтқан қайраткер?\"\n",
    "check_converted_model_generation(\"/data/nvme6n1p1/adal_workspace/small_llm/models/chatting_llm/try_1/checkpoint-566\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROMPT = \"Сингапур туралы айтып берші?\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/nvme6n1p1/adal_workspace/small_llm/models/chatting_llm/try_1/checkpoint-566\")\n",
    "messages = [{'role':'user', 'content':TEST_PROMPT}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "formatted_prompt_2 = formatted_prompt.replace(\"user : \", \"user:\\n\") + \"<|im_start|>assistant:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сингапур - Оңтүстік-Шығыс Азиядағы аралдық мемлекет. Ол 1963 жылы 9 желтоқсанда Ұлыбританиядан тәуелсіздік алды. Ол өзінің ерекше мәдениеті мен тілімен, таңғажайып жағажайларымен және жанды түнгі өмірімен танымал.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://0.0.0.0:8009/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"/data/nvme6n1p1/adal_workspace/small_llm/models/chatting_llm/try_1/checkpoint-566\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "    ],\n",
    "  stream=True,\n",
    "  temperature=0.4,\n",
    "  top_p=0.95,\n",
    "  stop=[\"<|endoftext|>\",\"<|im_end|>\", \"<|im_start|>\"],\n",
    "    extra_body={\n",
    "        \"skip_special_tokens\": False,\n",
    "        # \"repetition_penalty\":1.17\n",
    "    },\n",
    ")\n",
    "cool = []\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        cool.append(chunk.choices[0].delta.content)\n",
    "\n",
    "print(\"\".join(cool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
